#!/usr/bin/env bash

set -e

#################################
####### Prepare Variables #######
#################################

# Default cron has limited path and nodetool and
export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

bucket=gs://{{ bucket_name }}
snap_date="$(date +"%y-%m-%d")"

base={{ data_directory }}
host=$(hostname)


empty_file=/tmp/empty
done_msg=BACKUP_DONE_SUCCESSFULLY

metrics_dir=/var/lib/node-exporter/textfile-collector
mkdir -p -m 777 $metrics_dir
metrics_file=${metrics_dir}/cassandra_backup.prom
metrics_tmpl='
# TYPE cassandra_backup counter
# HELP cassandra_backup Counts the number of successful backups
cassandra_backup{type="snapshot",errno="%d"} 1 %d
# TYPE cassandra_backup_duration_seconds gauge
# HELP cassandra_backup_duration_seconds Track the duration of the backup task, in seconds
cassandra_backup_duration_seconds{type="snapshot",errno="%s"} %d %d
'

started_at=$(date +%s%3N)

#################################
##### Cleanup Trap ##############
#################################

function cleanup {
  local errno=$?

  # Make sure we clear the snapshot we created, even if we didn't succeed
  # uploading it to GCS. Snaphots occupy a lot of disk space, and there is no
  # benefit in keeping them locally: if the instance (and it's ephemeral
  # storage) still exists, there is no reason to restore it from a snapshot.
  nodetool clearsnapshot --all || true
  now=$(date +%s%3N)
  # nb. the date must be milliseconds since the epoch
  printf "$metrics_tmpl" $errno "$now" $errno $(((now-started_at)/1000)) "$now" > ${metrics_file}.$$ \
    && mv -v ${metrics_file}.$$ ${metrics_file} || true

  exit $errno
}
trap cleanup EXIT

#################################
##### Snapshot and Upload  ######
#################################

# Only occupy CPU #1
taskset -p 0x00000001 $$ > /dev/null 2>&1

# Clean _all_ old snapshots, just in case
nodetool clearsnapshot --all

# Create a new snapshot
echo "Creating snapshot"
nodetool snapshot -t "$snap_date"

echo "Copy files to GCS"
running_processes=0

# Upload to GCS
for snapshot_subdir in $base/*/*/snapshots/$snap_date/; do
    table_dir=$(dirname $(dirname $snapshot_subdir))
    table=$(basename $table_dir | cut -d '-' -f1)
    keyspace=$(basename $(dirname $table_dir))
    # Do not backup system tables
    if [[ ! $keyspace =~ "system" ]]; then
       sudo gcloud storage cp -r $snapshot_subdir $bucket/$snap_date/$host/$keyspace/$table &

       running_processes=$((running_processes+1))

       # Allow a maximum of 5 copy jobs in the background in parallel. If those are busy wait for all to finish before starting new copy jobs
       if [[ "$running_processes" -eq 5 ]]; then
         echo "Waiting for ${running_processes} copy processes to finish"
         wait
         running_processes=0
       fi
    fi
done

echo "Wait for all jobs to finish"
wait

echo "Done Copying files to GCS"

#################################
#### Mark remote as done  #######
#################################

# Stamp of approval
touch $empty_file
sudo gsutil cp "$empty_file" "$bucket/$snap_date/$host/$done_msg"